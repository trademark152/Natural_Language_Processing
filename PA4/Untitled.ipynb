{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\trade\\Anaconda3\\envs\\MinhEnv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import re  # regular expression operations\n",
    "\n",
    "'''Return all text file in the given directory'''\n",
    "def GetInputFiles():\n",
    "    # print(glob.glob(os.path.join(sys.argv[1], '*/*/*/*.txt')))\n",
    "    return glob.glob(os.path.join(sys.argv[1], '*/*/*/*.txt'))\n",
    "\n",
    "\"\"\" DEBUG TIPS: \"\"\"\n",
    "VARS = {}\n",
    "\n",
    "# vocab = {word 1: count 1}\n",
    "VOCABULARY = collections.Counter()\n",
    "stopWords = [\"each\",\"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\",\"and\", \"but\", \"if\",  \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"or\", \"because\", \"as\", \"until\", \"while\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\",  \"of\", \"at\", \"by\", \"for\", \"with\",  \"after\", \"above\",\"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"have\",  \"a\", \"an\", \"the\",  \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\",  \"nor\",  \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\"not\", \"only\", \"own\", \"same\",\"don\", \"should\", \"now\"]\n",
    "\n",
    "# remove words from stopWords\n",
    "def removeStops(tokenList):\n",
    "    output = []\n",
    "    # loop through each token to see if it is in the drop list\n",
    "    for token in tokenList:\n",
    "        if token in stopWords:\n",
    "            # do nothing\n",
    "            continue\n",
    "        else:\n",
    "            # return token not in stopWords\n",
    "            output.append(token)\n",
    "    return output\n",
    "\n",
    "# Function to remove punctuation from a token list []\n",
    "def removePunctuations(tokenList):\n",
    "    output = []\n",
    "    # loop through each token\n",
    "    for token in tokenList:\n",
    "        # to do a regex substitution: remove all except letter, and inherent white spaces\n",
    "        token = re.sub('[^a-zA-Z0-9\\n\\-]', '', token)\n",
    "        output.append(token)\n",
    "    return output\n",
    "\n",
    "# Function to remove short words from a token list []\n",
    "def removeShortTokens(tokenList):\n",
    "    output = []\n",
    "    # loop through each token\n",
    "    for token in tokenList:\n",
    "        # to do a regex substitution: remove all except letter, number and inherent white spaces\n",
    "        if len(token) >= 2.0:\n",
    "            output.append(token)\n",
    "    return output\n",
    "\n",
    "# ** TASK 1.\n",
    "def Tokenize(comment):\n",
    "    # split into words by white space\n",
    "    tokens = re.split('[^a-zA-Z]', comment)\n",
    "\n",
    "    # remove punctuations of each words\n",
    "    # tokens = removePunctuations(tokens)\n",
    "\n",
    "    # remove stop word\n",
    "    # tokens = removeStops(tokens)\n",
    "\n",
    "    # put them to lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # # remove punctuation from each word\n",
    "    # table = str.maketrans('', '', string.punctuation)\n",
    "    # strippedTokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "    # remove short word\n",
    "    finalTokens = removeShortTokens(tokens)\n",
    "    # print(finalTokens[:100])\n",
    "    return finalTokens\n",
    "\n",
    "\n",
    "\n",
    "# ** TASK 2.\n",
    "def FirstLayer(net, l2_reg_val, is_training):\n",
    "    \"\"\"First layer of the neural network.\n",
    "\n",
    "    Args:\n",
    "      net: 2D tensor (batch-size, number of vocabulary tokens),\n",
    "      l2_reg_val: float -- regularization coefficient.\n",
    "      is_training: boolean tensor.A\n",
    "\n",
    "    Returns:\n",
    "      2D tensor (batch-size, 40), where 40 is the hidden dimensionality.??\n",
    "    \"\"\"\n",
    "\n",
    "    # To do:\n",
    "    # replace RELU with tanh\n",
    "    # remove bias vector\n",
    "    # Replace the L2-regularization of fully connected with manual regularization.\n",
    "    # Preprocess the layer input by passing\n",
    "    # Add Batch Normalization.\n",
    "\n",
    "    global VARS\n",
    "    ## Specify regularizer\n",
    "    # Returns a function that can be used to apply L2 regularization to weights.\n",
    "    # l2_reg = tf.contrib.layers.l2_regularizer(l2_reg_val)\n",
    "    VARS[\"a\"] = net  # X\n",
    "    # print(\"first net: \", VARS[\"a\"])\n",
    "\n",
    "    ## Normalization row-wise for each data row with L2 norm\n",
    "    net = tf.nn.l2_normalize(net, axis=1)\n",
    "    VARS[\"b\"] =net  # X/Xnorm\n",
    "    # print(\"2nd net after l2 normalize: \", VARS[\"b\"])\n",
    "\n",
    "    ## Adds a fully connected layer: with input,\n",
    "    # Output: weight matrix net\n",
    "\n",
    "    net = tf.contrib.layers.fully_connected(\n",
    "        net, 40, activation_fn=None, weights_regularizer=None, biases_initializer=None)\n",
    "    VARS[\"c\"] = net  # XY/Xnorm\n",
    "    # print(\"third net after adding layer: \", VARS[\"c\"])\n",
    "\n",
    "    ## Because net = X*Y so Y = (X)^-1 * net\n",
    "    loss_reg = l2_reg_val * net ** 2.0\n",
    "    tf.losses.add_loss(tf.reduce_sum(loss_reg), loss_collection=tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "\n",
    "    ## Add batch norm\n",
    "    net = tf.contrib.layers.batch_norm(net)\n",
    "    VARS[\"d\"] = net\n",
    "    # print(\"4th net after batch norm \", VARS[\"d\"])\n",
    "\n",
    "    ## Activation:??\n",
    "    net = tf.nn.tanh(net)\n",
    "    VARS[\"e\"] = net\n",
    "    # print(\"5th net after tanh activation \", VARS[\"e\"])\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "# ** TASK 2 ** BONUS part 1\n",
    "def EmbeddingL2RegularizationUpdate(embedding_variable, net_input, learn_rate, l2_reg_val):\n",
    "    \"\"\"Accepts tf.Variable, tensor (batch_size, vocab size), regularization coef.\n",
    "    Returns tf op that applies one regularization step on embedding_variable.\"\"\"\n",
    "    # TODO(student): Change this to something useful. Currently, this is a no-op.\n",
    "    # normalized_input = tf.nn.l2_normalize(net_input, axis=1)\n",
    "    # # update_diff = learn_rate * (2*l2_reg_val * tf.matmul(embedding_variable, normalized_input))\n",
    "    # return embedding_variable.assign(embedding_variable, embedding_variable - learn_rate * (2*l2_reg_val * tf.matmul(embedding_variable, normalized_input)))\n",
    "    return embedding_variable.assign(embedding_variable)\n",
    "\n",
    "\n",
    "# ** TASK 2 ** BONUS part 2\n",
    "def EmbeddingL1RegularizationUpdate(embedding_variable, net_input, learn_rate, l1_reg_val):\n",
    "    \"\"\"Accepts tf.Variable, tensor (batch_size, vocab size), regularization coef.\n",
    "    Returns tf op that applies one regularization step on embedding_variable.\"\"\"\n",
    "    # TODO(student): Change this to something useful. Currently, this is a no-op.\n",
    "    # return embedding_variable.assign(embedding_variable, embedding_variable- learn_rate * embedding_variable)\n",
    "    return embedding_variable.assign(embedding_variable)\n",
    "\n",
    "\n",
    "# ** TASK 3\n",
    "def SparseDropout(slice_x, keep_prob=0.5):\n",
    "    \"\"\"Sets random (1 - keep_prob) non-zero elements of slice_x to zero.\n",
    "\n",
    "    Args:\n",
    "      slice_x: 2D numpy array (batch_size, vocab_size)\n",
    "\n",
    "    Returns:\n",
    "      2D numpy array (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    # Get indices of non-zero elements:\n",
    "    i, j = numpy.nonzero(slice_x)\n",
    "\n",
    "    # Get random indices to set to zero\n",
    "    indices = numpy.random.choice(len(i), int(numpy.floor((1-keep_prob) * len(i))), replace=False)\n",
    "\n",
    "    # set the non-zero values at these random indices to zero\n",
    "    slice_x[i[indices], j[indices]] =0\n",
    "\n",
    "    return slice_x\n",
    "\n",
    "\n",
    "# ** TASK 4\n",
    "# TODO(student): YOU MUST SET THIS TO GET CREDIT.\n",
    "# You should set it to tf.Variable of shape (vocabulary, 40).\n",
    "EMBEDDING_VAR = tf.Variable(tf.zeros([len(VOCABULARY), 40]), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# ** TASK 5\n",
    "# This is called automatically by VisualizeTSNE.\n",
    "#  t-distributed stochastic neighbor embedding\n",
    "def ComputeTSNE(embedding_matrix):\n",
    "    \"\"\"Projects embeddings onto 2D by computing tSNE.\n",
    "\n",
    "    Args:\n",
    "      embedding_matrix: numpy array of size (vocabulary, 40)\n",
    "\n",
    "    Returns:\n",
    "      numpy array of size (vocabulary, 2)\n",
    "    \"\"\"\n",
    "    from sklearn.manifold import TSNE\n",
    "    embedding_matrix_new = TSNE(n_components=2).fit_transform(embedding_matrix)\n",
    "    # print(\"embedding matrix: \", embedding_matrix_new[:, :2])\n",
    "    return embedding_matrix_new[:, :2]\n",
    "\n",
    "\n",
    "# ** TASK 5\n",
    "# This should save a PDF of the embeddings. This is the *only* function marked\n",
    "# marked with \"** TASK\" that will NOT be automatically invoked by our grading\n",
    "# script (it will be \"stubbed-out\", by monkey-patching). You must run this\n",
    "# function on your own, save the PDF produced by it, and place it in your\n",
    "# submission directory with name 'tsne_embeds.pdf'.\n",
    "def VisualizeTSNE(sess):\n",
    "    if EMBEDDING_VAR is None:\n",
    "        print('Cannot visualize embeddings. EMBEDDING_VAR is not set')\n",
    "        return\n",
    "    embedding_mat = sess.run(EMBEDDING_VAR)\n",
    "    tsne_embeddings = ComputeTSNE(embedding_mat)\n",
    "    # print(\"tsne embeddings: \", tsne_embeddings)\n",
    "\n",
    "    class_to_words = {\n",
    "        'positive': [\n",
    "            'relaxing', 'upscale', 'luxury', 'luxurious', 'recommend', 'relax',\n",
    "            'choice', 'best', 'pleasant', 'incredible', 'magnificent',\n",
    "            'superb', 'perfect', 'fantastic', 'polite', 'gorgeous', 'beautiful',\n",
    "            'elegant', 'spacious'\n",
    "        ],\n",
    "        'location': [\n",
    "            'avenue', 'block', 'blocks', 'doorman', 'windows', 'concierge', 'living'\n",
    "        ],\n",
    "        'furniture': [\n",
    "            'bedroom', 'floor', 'table', 'coffee', 'window', 'bathroom', 'bath',\n",
    "            'pillow', 'couch'\n",
    "        ],\n",
    "        'negative': [\n",
    "            'dirty', 'rude', 'uncomfortable', 'unfortunately', 'ridiculous',\n",
    "            'disappointment', 'terrible', 'worst', 'mediocre'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    # TODO(student): Visualize scatter plot of tsne_embeddings, showing only words\n",
    "    # listed in class_to_words. Words under the same class must be visualized with\n",
    "    # the same color. Plot both the word text and the tSNE coordinates.\n",
    "\n",
    "    # print(\"Term index: \", TERM_INDEX)\n",
    "    # print(\"Vocabulary: \", VOCABULARY)\n",
    "\n",
    "    # need to extract 2 list:\n",
    "    # labels: all the words in class_to_words\n",
    "    # sub_tsne_embeddings: part of tsne_embeddings corresponding to those words in labels\n",
    "    labels = []\n",
    "    new_values = []\n",
    "    classes = []\n",
    "\n",
    "    for cluster in class_to_words:\n",
    "        for word in class_to_words[cluster]:\n",
    "            # add words and its corresponding tsne\n",
    "            labels.append(word)\n",
    "            classes.append(cluster)\n",
    "            new_values.append(tsne_embeddings[TERM_INDEX[word]])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as colors\n",
    "    import matplotlib.cm as cmx\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    # Set the color map to match the number of species\n",
    "    z = range(1, len(class_to_words))\n",
    "    hot = plt.get_cmap('hot')\n",
    "    cNorm = colors.Normalize(vmin=0, vmax=len(class_to_words))\n",
    "    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=hot)\n",
    "\n",
    "    f=plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i], s=15, color=scalarMap.to_rgba(i))\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.title('Selected Word Embedding TSNE')\n",
    "    plt.show()\n",
    "    print('visualization should generate now')\n",
    "\n",
    "    # save the plot to pdf file\n",
    "    f.savefig(\"tsne_embeds.pdf\", bbox_inches='tight')\n",
    "\n",
    "CACHE = {}\n",
    "\n",
    "'''Read and tokenize a file with fileName'''\n",
    "def ReadAndTokenize(filename):\n",
    "    \"\"\"return dict containing of terms to frequency.\"\"\"\n",
    "    global CACHE\n",
    "    global VOCABULARY\n",
    "\n",
    "    # search if file is already in CACHE\n",
    "    if filename in CACHE:\n",
    "        return CACHE[filename]\n",
    "\n",
    "    # open content of the file\n",
    "    comment = open(filename).read()\n",
    "\n",
    "    # tokenize into list of words\n",
    "    words = Tokenize(comment)\n",
    "\n",
    "    # counting appearance of each word of vocab in words\n",
    "    terms = collections.Counter()\n",
    "\n",
    "    # loop through each word\n",
    "    for w in words:\n",
    "        # update the count and vocab\n",
    "        VOCABULARY[w] += 1\n",
    "        terms[w] += 1\n",
    "\n",
    "    # update CACHE\n",
    "    CACHE[filename] = terms\n",
    "    return terms\n",
    "\n",
    "# This global variable is used to track {word1: index1}\n",
    "TERM_INDEX = None\n",
    "\n",
    "'''Part of word embedding process'''\n",
    "### input: X: [terms1, terms2,...] with terms1 representing doc1 = {word1:count1, word2:count2]\n",
    "### output: X_matrix: #doc*#features\n",
    "def MakeDesignMatrix(x):\n",
    "    global TERM_INDEX\n",
    "    if TERM_INDEX is None:\n",
    "        print('Total words: %i' % len(VOCABULARY.values()))\n",
    "\n",
    "        # Returns the q-th percentile(s) of the array elements.\n",
    "        # min_count is more like median count because of sparse data??\n",
    "        min_count, max_count = numpy.percentile(list(VOCABULARY.values()), [50.0, 99.8])\n",
    "        # print(\"min_count \", min_count)\n",
    "        # print(\"max_count \", max_count)\n",
    "\n",
    "        # only perform embedding when word frequency reaches certain threshold >50th percentile\n",
    "        TERM_INDEX = {}\n",
    "        for term, count in VOCABULARY.items():\n",
    "            if count > min_count and count <= max_count:\n",
    "                # add terms sequentially with their index\n",
    "                idx = len(TERM_INDEX)\n",
    "                TERM_INDEX[term] = idx\n",
    "\n",
    "    # initiate x_matrix\n",
    "    x_matrix = numpy.zeros(shape=[len(x), len(TERM_INDEX)], dtype='float32')\n",
    "\n",
    "    # loop through x with x = [doc1,doc2...] and doc1={token1:count1, token2:count2,...}\n",
    "    for i, item in enumerate(x):\n",
    "        # loop through each token and its count\n",
    "        for term, count in item.items():\n",
    "            if term not in TERM_INDEX:\n",
    "                continue\n",
    "\n",
    "            # get the necessary index of each term\n",
    "            j = TERM_INDEX[term]\n",
    "\n",
    "            # update the count in x_matrix\n",
    "            x_matrix[i, j] = count  # 1.0  # Try count or log(1+count)\n",
    "    return x_matrix\n",
    "\n",
    "'''Construct train and test data from all text files and make matrices'''\n",
    "def GetDataset():\n",
    "    \"\"\"Returns numpy arrays of training and testing data.\"\"\"\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "\n",
    "    classes1 = set()\n",
    "    classes2 = set()\n",
    "\n",
    "    # loop through text files from the directory\n",
    "    for f in GetInputFiles():\n",
    "        # print(f)\n",
    "        # print(f.split('\\\\')[-4:])\n",
    "\n",
    "        # extract each class (Truthful/Deceptive, Positive/Negative), data fold and file name\n",
    "        class1, class2, fold, fname = f.split('\\\\')[-4:]\n",
    "        classes1.add(class1)\n",
    "        classes2.add(class2)\n",
    "        class1 = class1.split('_')[0] #??\n",
    "        class2 = class2.split('_')[0]\n",
    "\n",
    "        # read and tokenize each text file\n",
    "        x = ReadAndTokenize(f)\n",
    "\n",
    "        # y is a list [1,1] for positive and truthful...\n",
    "        y = [int(class1 == 'positive'), int(class2 == 'truthful')]\n",
    "\n",
    "        # save fold 4 for testing\n",
    "        if fold == 'fold4':\n",
    "            x_test.append(x)\n",
    "            y_test.append(y)\n",
    "        # add the rest to train data\n",
    "        else:\n",
    "            x_train.append(x)\n",
    "            y_train.append(y)\n",
    "\n",
    "    ### Make numpy arrays: transform train and test data to matrices\n",
    "    x_test = MakeDesignMatrix(x_test) # numDocs*numFeatures\n",
    "    x_train = MakeDesignMatrix(x_train)\n",
    "    y_test = numpy.array(y_test, dtype='float32') # numDocs*numClasses\n",
    "    y_train = numpy.array(y_train, dtype='float32')\n",
    "\n",
    "    # combine to dataset as pickle\n",
    "    dataset = (x_train, y_train, x_test, y_test)\n",
    "\n",
    "    # write binary this dataset\n",
    "    with open('dataset.pkl', 'wb') as fout:\n",
    "        pickle.dump(dataset, fout)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "'''print out evaluation results'''\n",
    "def print_f1_measures(probs, y_test):\n",
    "    y_test[:, 0] == 1  # Positive\n",
    "    positive = {\n",
    "        'tp': numpy.sum((probs[:, 0] > 0)[numpy.nonzero(y_test[:, 0] == 1)[0]]),\n",
    "        'fp': numpy.sum((probs[:, 0] > 0)[numpy.nonzero(y_test[:, 0] == 0)[0]]),\n",
    "        'fn': numpy.sum((probs[:, 0] <= 0)[numpy.nonzero(y_test[:, 0] == 1)[0]]),\n",
    "    }\n",
    "    negative = {\n",
    "        'tp': numpy.sum((probs[:, 0] <= 0)[numpy.nonzero(y_test[:, 0] == 0)[0]]),\n",
    "        'fp': numpy.sum((probs[:, 0] <= 0)[numpy.nonzero(y_test[:, 0] == 1)[0]]),\n",
    "        'fn': numpy.sum((probs[:, 0] > 0)[numpy.nonzero(y_test[:, 0] == 0)[0]]),\n",
    "    }\n",
    "    truthful = {\n",
    "        'tp': numpy.sum((probs[:, 1] > 0)[numpy.nonzero(y_test[:, 1] == 1)[0]]),\n",
    "        'fp': numpy.sum((probs[:, 1] > 0)[numpy.nonzero(y_test[:, 1] == 0)[0]]),\n",
    "        'fn': numpy.sum((probs[:, 1] <= 0)[numpy.nonzero(y_test[:, 1] == 1)[0]]),\n",
    "    }\n",
    "    deceptive = {\n",
    "        'tp': numpy.sum((probs[:, 1] <= 0)[numpy.nonzero(y_test[:, 1] == 0)[0]]),\n",
    "        'fp': numpy.sum((probs[:, 1] <= 0)[numpy.nonzero(y_test[:, 1] == 1)[0]]),\n",
    "        'fn': numpy.sum((probs[:, 1] > 0)[numpy.nonzero(y_test[:, 1] == 0)[0]]),\n",
    "    }\n",
    "\n",
    "    all_f1 = []\n",
    "    for attribute_name, score in [('truthful', truthful),\n",
    "                                  ('deceptive', deceptive),\n",
    "                                  ('positive', positive),\n",
    "                                  ('negative', negative)]:\n",
    "        precision = float(score['tp']) / float(score['tp'] + score['fp'])\n",
    "        recall = float(score['tp']) / float(score['tp'] + score['fn'])\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        all_f1.append(f1)\n",
    "        print('{0:9} {1:.2f} {2:.2f} {3:.2f}'.format(attribute_name, precision, recall, f1))\n",
    "    print('Mean F1: {0:.4f}'.format(float(sum(all_f1)) / len(all_f1)))\n",
    "\n",
    "''' Construct neural network'''\n",
    "def BuildInferenceNetwork(x, l2_reg_val, is_training):\n",
    "    \"\"\"From a tensor x, runs the neural network forward to compute outputs.\n",
    "    This essentially instantiates the network and all its parameters.\n",
    "\n",
    "    Args:\n",
    "      x: Tensor of shape (batch_size, vocab size) which contains a sparse matrix\n",
    "         where each row is a training example and containing counts of words\n",
    "         in the document that are known by the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "      Tensor of shape (batch_size, 2) where the 2-columns represent class\n",
    "      memberships: one column discriminates between (negative and positive) and\n",
    "      the other discriminates between (deceptive and truthful).\n",
    "    \"\"\"\n",
    "    global EMBEDDING_VAR\n",
    "    EMBEDDING_VAR = None # ** TASK 4: Move and set appropriately.\n",
    "    # print(\"Embedding var: \", EMBEDDING_VAR)\n",
    "\n",
    "    ## Build layers starting from input.\n",
    "    net = x\n",
    "\n",
    "    # get L2 regularizer value\n",
    "    l2_reg = tf.contrib.layers.l2_regularizer(l2_reg_val)\n",
    "\n",
    "    # print(\"trainable variables before first layer: \", tf.trainable_variables())\n",
    "\n",
    "    ## First Layer\n",
    "    net = FirstLayer(net, l2_reg_val, is_training)\n",
    "    EMBEDDING_VAR = [v for v in tf.global_variables() if v.name == \"fully_connected/weights:0\"][0]\n",
    "    # print(\"Embedding var after first layer: \", EMBEDDING_VAR)\n",
    "\n",
    "    # print(\"trainable variables after first layer: \", tf.trainable_variables())\n",
    "\n",
    "    ## Second Layer.\n",
    "    # create a fully connected layer:\n",
    "    net = tf.contrib.layers.fully_connected(\n",
    "        net, 10, activation_fn=None, weights_regularizer=l2_reg)\n",
    "    EMBEDDING_VAR = [v for v in tf.global_variables() if v.name == \"fully_connected/weights:0\"][0]\n",
    "    # print(\"Embedding var after second layer: \", EMBEDDING_VAR)\n",
    "\n",
    "    # perform dropout\n",
    "    net = tf.contrib.layers.dropout(net, keep_prob=0.5, is_training=is_training)\n",
    "    EMBEDDING_VAR = [v for v in tf.global_variables() if v.name == \"fully_connected/weights:0\"][0]\n",
    "    # print(\"Embedding var after DROPOUT: \", EMBEDDING_VAR)\n",
    "\n",
    "    # perform activation function\n",
    "    net = tf.nn.relu(net)\n",
    "    EMBEDDING_VAR = [v for v in tf.global_variables() if v.name == \"fully_connected/weights:0\"][0]\n",
    "    # print(\"Embedding var after second layer's activation: \", EMBEDDING_VAR)\n",
    "\n",
    "    ## Third Layer\n",
    "    net = tf.contrib.layers.fully_connected(\n",
    "        net, 2, activation_fn=None, weights_regularizer=l2_reg)\n",
    "    EMBEDDING_VAR = [v for v in tf.global_variables() if v.name == \"fully_connected/weights:0\"][0]\n",
    "    # print(\"Embedding var after third and final layer: \", EMBEDDING_VAR)\n",
    "\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MAIN with argument'''\n",
    "def main(argv):\n",
    "    ######### Read dataset\n",
    "    x_train, y_train, x_test, y_test = GetDataset()\n",
    "\n",
    "    ######### Neural Network Model\n",
    "    # set placeholders the same size with test data: X is a matrix of input features (# docs* features)\n",
    "    x = tf.placeholder(tf.float32, [None, x_test.shape[1]], name='x')\n",
    "\n",
    "    # target matrix Y: # docs * numClassses\n",
    "    y = tf.placeholder(tf.float32, [None, y_test.shape[1]], name='y')\n",
    "\n",
    "    is_training = tf.placeholder(tf.bool, [])  #boolean tensor\n",
    "\n",
    "    # Co-efficient for L2 regularization (lambda)\n",
    "    l2_reg_val = 1e-6\n",
    "\n",
    "    # Build inference network based on training data, regularization coefficient and boolean is_training\n",
    "    net = BuildInferenceNetwork(x, l2_reg_val, is_training)\n",
    "\n",
    "    ######### Loss Function:\n",
    "    tf.losses.sigmoid_cross_entropy(multi_class_labels=y, logits=net)\n",
    "\n",
    "    ######### Training Algorithm\n",
    "    # learning rate\n",
    "    learning_rate = tf.placeholder_with_default(\n",
    "        numpy.array(0.01, dtype='float32'), shape=[], name='learn_rate')\n",
    "\n",
    "    # optimizer is gradient descent optimizer\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # training optimizer\n",
    "    train_op = tf.contrib.training.create_train_op(tf.losses.get_total_loss(), opt)\n",
    "\n",
    "    # Run a session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # FOR DEBUGGING\n",
    "    global VARS\n",
    "    # import IPython;  IPython.embed()\n",
    "\n",
    "    # Function to evaluate on a batch of sample\n",
    "    def evaluate(batch_x=x_test, batch_y=y_test):\n",
    "        probs = sess.run(net, {x: batch_x, is_training: False})\n",
    "        print_f1_measures(probs, batch_y)\n",
    "\n",
    "    # Function to learn on a batch of sample with learning rate lr\n",
    "    def batch_step(batch_x, batch_y, lr):\n",
    "        sess.run(train_op, {\n",
    "            x: batch_x,\n",
    "            y: batch_y,\n",
    "            is_training: True, learning_rate: lr,\n",
    "        })\n",
    "\n",
    "    # randomly slice training data based on batch_size and lr\n",
    "    def step(lr=0.01, batch_size=100):\n",
    "        indices = numpy.random.permutation(x_train.shape[0])\n",
    "        for si in range(0, x_train.shape[0], batch_size):\n",
    "            se = min(si + batch_size, x_train.shape[0])\n",
    "            slice_x = x_train[indices[si:se]] + 0  # + 0 to copy slice\n",
    "\n",
    "            # perform sparse dropout\n",
    "            slice_x = SparseDropout(slice_x)\n",
    "\n",
    "            # Get a batch of training data\n",
    "            batch_step(slice_x, y_train[indices[si:se]], lr)\n",
    "\n",
    "    lr = 0.05\n",
    "    print('Training model ... ')\n",
    "    for j in range(300): step(lr)\n",
    "    for j in range(300): step(lr / 2)\n",
    "    for j in range(300): step(lr / 4)\n",
    "    print('Results from training:')\n",
    "    evaluate()\n",
    "\n",
    "    #### Save parameters:\n",
    "    # var_dict = {v.name: v for v in tf.global_variables()}\n",
    "    # pickle.dump(sess.run(var_dict), open('trained_vars.pkl', 'w'))\n",
    "\n",
    "    #### Restore parameters for prediction\n",
    "    # var_values = pickle.load(open('trained_vars.pkl'))\n",
    "    # assign_ops = [v.assign(var_values[v.name]) for v in tf.global_variables()]\n",
    "    # sess.run(assign_ops)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "cannot do a non-empty take from an empty axes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fe5e0efa1264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# run main\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-ab47282bc7db>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(argv)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m######### Read dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m######### Neural Network Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-69aa8b051da6>\u001b[0m in \u001b[0;36mGetDataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[1;31m### Make numpy arrays: transform train and test data to matrices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMakeDesignMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# numDocs*numFeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMakeDesignMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# numDocs*numClasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-69aa8b051da6>\u001b[0m in \u001b[0;36mMakeDesignMatrix\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;31m# Returns the q-th percentile(s) of the array elements.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;31m# min_count is more like median count because of sparse data??\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVOCABULARY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m50.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m99.8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[1;31m# print(\"min_count \", min_count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;31m# print(\"max_count \", max_count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MinhEnv\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mpercentile\u001b[1;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[0;32m   3705\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Percentiles must be in the range [0, 100]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3706\u001b[0m     return _quantile_unchecked(\n\u001b[1;32m-> 3707\u001b[1;33m         a, q, axis, out, overwrite_input, interpolation, keepdims)\n\u001b[0m\u001b[0;32m   3708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MinhEnv\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_quantile_unchecked\u001b[1;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[0;32m   3824\u001b[0m     r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,\n\u001b[0;32m   3825\u001b[0m                     \u001b[0moverwrite_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3826\u001b[1;33m                     interpolation=interpolation)\n\u001b[0m\u001b[0;32m   3827\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3828\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MinhEnv\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3405\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3406\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MinhEnv\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_quantile_ureduce_func\u001b[1;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[0;32m   3939\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3941\u001b[1;33m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_below\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights_below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3942\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_above\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights_above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MinhEnv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(a, indices, axis, out, mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m            [5, 7]])\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'take'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MinhEnv\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: cannot do a non-empty take from an empty axes."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # set random seed\n",
    "    tf.random.set_random_seed(0)\n",
    "\n",
    "    # run main\n",
    "    main([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize learning process\n",
    "VisualizeTSNE(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
